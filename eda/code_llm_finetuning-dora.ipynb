{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f44884d-38c7-4656-a45e-b1b96afe7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Handle imports\n",
    "##############################################################################\n",
    "import traceback\n",
    "from collections.abc import Iterable\n",
    "import os\n",
    "import pypdfium2 as pdfium\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import uuid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval, AnswerRelevancyMetric\n",
    "from deepeval import evaluate\n",
    "from huggingface_hub import snapshot_download, login, HfApi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import xml.etree.ElementTree as etree\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "nltk.download('punkt_tab')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, EvalPrediction\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import EarlyStoppingCallback\n",
    "import torch\n",
    "import optuna\n",
    "import traceback\n",
    "import evaluate\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2df5d2-b6ab-41a2-bdcc-398fd85c45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Set up variables\n",
    "##############################################################################\n",
    "SOURCE_DIR=\"source_docs\"\n",
    "SOURCE_DIR_CHUNKED=\"source_docs_chunked\"\n",
    "MARKDOWN_DIR=\"markdown\"\n",
    "MARKDOWN_URI_PREFIX=\"https://raw.githubusercontent.com/agapebondservant/code-generation-capstone/refs/heads/main/eda/resources\"\n",
    "REPORT_DIR=\"reports\"\n",
    "OUTPUT_DIR=\"output\"\n",
    "INVALID_DIR=\"invalid\"\n",
    "ERROR_DIR=\"error\" \n",
    "MODEL_DIR=\"models\"\n",
    "# MODEL_IDS = [\"ibm-granite/granite-8b-code-instruct-4k\",\"ibm-granite/granite-8b-instruct-base-128k\"]\n",
    "MODEL_IDS = [\"ibm-granite/granite-4.0-h-tiny\"]\n",
    "DEVICE=\"cuda\"\n",
    "DATASET_REPO=f\"{os.getenv('HF_USERNAME')}/codegen\"\n",
    "EVAL_DIR=\"evals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146bed40-2949-4e17-8bc1-745b44393567",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Set up object instances\n",
    "##############################################################################\n",
    "\n",
    "data_generator_llm = ChatOpenAI(\n",
    "    model=os.getenv(\"DATA_GENERATOR_MODEL_ID\"), # os.getenv('QWEN25CODER_MODEL_ID'),\n",
    "    api_key=os.getenv('OPENROUTER_TOKEN'),\n",
    "    base_url=os.getenv('OPENROUTER_API_BASE'),\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "class DataGeneratorLLM(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Data Generator LLM (GPT-OSS)\"\n",
    "\n",
    "evaluator_llm = DataGeneratorLLM(data_generator_llm)\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa50c10-3803-4a75-b143-4efc12f75988",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PROMPTS AND PROMPT TYPES\n",
    "##############################################################################\n",
    "\n",
    "summary_prompt = \"\"\"\n",
    "Your task is to analyze this code snippet and provide an explanation of the code.\n",
    "    \n",
    "Instructions:\n",
    "1. Provide a concise explanation that summarizes the purpose of the code without getting into too many specific technical details.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "3. Also exclude any details that link the requirements to a specific programming language or framework.\n",
    "\"\"\"\n",
    "\n",
    "topics_prompt = \"\"\"\n",
    "Use the provided summary to analyze this code snippet and generate a list of programming topics that are related to the code.\n",
    "    \n",
    "Instructions:\n",
    "1. Provide a short list of topics that you can identify.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "components_prompt = \"\"\"\n",
    "Your task is to analyze this code snippet and generate a specification of all the JSP relevant components you can find.\n",
    "\n",
    "Instructions:\n",
    "1. Include only relevant components.\n",
    "3. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "domain_prompt = \"\"\"\n",
    "Your task is to analyze this code snippet and generate an outline of the domain model associated with this code.\n",
    "    \n",
    "Instructions:\n",
    "1. Avoid getting into too many specific technical details. Simply provide a domain model of the code.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "3. Include the current state of the domain objects based on information extracted from the code.\n",
    "\"\"\"\n",
    "\n",
    "keywords_prompt = \"\"\"\n",
    "Your task is to analyze this code snippet and generate a list of keywords that are associated with the code.\n",
    "    \n",
    "Instructions:\n",
    "1. Provide a short list of keywords.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "functional_requirements_prompt = \"\"\"\n",
    "Use the provided summary to analyze this code snippet and generate a list of programming topics that are related to the code.\n",
    "    \n",
    "Instructions:\n",
    "1. Provide a short list of topics that you can identify.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "business_requirements_prompt = \"\"\"\n",
    "Use the provided summary to generate an outline of sample business requirements that might be connected to the code.\n",
    "\n",
    "Instructions:\n",
    "1. Provide a short list of relevant requirements. Do not include requirements that are not related to the code.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "prompts = {\n",
    "\n",
    "    \"functional_requirements\": {\n",
    "        \n",
    "        \"prompt\": functional_requirements_prompt, \n",
    "\n",
    "        \"title\": \"Functional Requirements\",\n",
    "    },\n",
    "    \"business_requirements\": {\n",
    "        \n",
    "        \"prompt\": business_requirements_prompt, \n",
    "\n",
    "        \"title\": \"Business Requirements\",\n",
    "    },\n",
    "    \"topics\": {\n",
    "        \n",
    "        \"prompt\": topics_prompt, \n",
    "\n",
    "        \"title\": \"Components\",\n",
    "    },\n",
    "    \"components\": {\n",
    "        \n",
    "        \"prompt\": components_prompt,\n",
    "\n",
    "        \"title\": \"Topics\",\n",
    "    },\n",
    "    \"keywords\": {\n",
    "        \n",
    "        \"prompt\": keywords_prompt,\n",
    "\n",
    "        \"title\": \"Keywords\",\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \n",
    "        \"prompt\": summary_prompt,\n",
    "\n",
    "        \"title\": \"Summary\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "prompts_with_dependencies = {\n",
    "    \"topics\": \"summary\",\n",
    "    \n",
    "    \"business_requirements\": \"summary\",\n",
    "    \n",
    "    \"functional_requirements\": \"summary\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a16f1a-8c36-4e70-b4f6-fb7ac24e54b0",
   "metadata": {},
   "source": [
    "### Download candidate models\n",
    "The following candidate models will be downloaded:\n",
    "- ibm-granite/granite-8b-code-instruct-4k\n",
    "- ibm-granite/granite-8b-code-base-128k\n",
    "- ibm-granite/granite-4.0-h-tiny\n",
    "\n",
    "(Of these, ibm-granite/granite-4.0-h-tiny will be selected for finetuning due to library compatibility issues with the other models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee5b701-d7ff-404c-87e4-c0893efd1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# UTILITY METHODS\n",
    "##############################################################################\n",
    "\n",
    "def download_models(repo_id):\n",
    "    try:\n",
    "        ##############################################################################\n",
    "        # Save the model\n",
    "        ##############################################################################\n",
    "        local_dir = snapshot_download(repo_id=repo_id, cache_dir=MODEL_DIR)\n",
    "        \n",
    "        print(f\"Model {repo_id} downloaded to: {local_dir}\")\n",
    "\n",
    "        ##############################################################################\n",
    "        # Save the tokenizer\n",
    "        ##############################################################################\n",
    "        tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            \n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        tokenizer.save_pretrained(local_dir)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "    \n",
    "        print(f\"Error downloading model {repo_id}: {e}\")\n",
    "\n",
    "def upload_models(repo_id, model_dir):\n",
    "\n",
    "    try:\n",
    "    \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_dir, \n",
    "                                                     trust_remote_code=True,\n",
    "                                                     device_map=DEVICE)\n",
    "    \n",
    "        api = HfApi()\n",
    "    \n",
    "        api.create_repo(repo_id=repo_id, repo_type=\"model\")\n",
    "    \n",
    "        api.upload_folder(\n",
    "            folder_path=model_dir,\n",
    "            \n",
    "            repo_id=repo_id,\n",
    "            \n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "    \n",
    "        print(f\"Error uploading model {repo_id} from directory {model_dir}: {e}\")\n",
    "\n",
    "def build_datasets(dataset_name):\n",
    "\n",
    "    final_datasets = []\n",
    "\n",
    "    def process_summary_to_text(example, code_type=\"\"):\n",
    "        \n",
    "        example[\"text\"], example[\"completion\"], example[\"code_type\"] = example[\"summary\"], example[code_type], [code_type]*len(example[\"code\"])\n",
    "        \n",
    "        return example\n",
    "\n",
    "    def process_code_to_text(example, code_type=\"\"):\n",
    "        example[\"text\"], example[\"completion\"], example[\"code_type\"] = example[\"code\"], example[code_type], [code_type]*len(example[\"code\"])\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    test_dataset = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "    for dataset in [train_dataset, test_dataset]:\n",
    "\n",
    "        datasets = []\n",
    "\n",
    "        code_types = [c for c, obj in prompts.items() if c not in ['code']]\n",
    "        \n",
    "        for code_type in code_types:\n",
    "\n",
    "            if code_type in prompts_with_dependencies:\n",
    "\n",
    "                datasets.append(dataset.map(process_summary_to_text, batched=True, fn_kwargs={\"code_type\": code_type}))\n",
    "            \n",
    "            else:\n",
    "\n",
    "                datasets.append(dataset.map(process_code_to_text, batched=True, fn_kwargs={\"code_type\": code_type}))\n",
    "\n",
    "        final_datasets.append(interleave_datasets(datasets))\n",
    "\n",
    "    return final_datasets\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d90482-ea95-4402-86c6-02a51d1bf92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Code Formatting Helper Function\n",
    "##############################################################################\n",
    "def code_text_formatter(example):\n",
    "\n",
    "    _code = example['code']\n",
    "    \n",
    "    _summary = example['summary']\n",
    "\n",
    "    _code_type = example[\"code_type\"]\n",
    "\n",
    "    _text = example['text']\n",
    "\n",
    "    _prompt = prompts[_code_type][\"prompt\"]\n",
    "\n",
    "    _title = prompts[_code_type][\"title\"]\n",
    "    \n",
    "    ######################################\n",
    "    # Code-Summary pair\n",
    "    ######################################\n",
    "    if _code_type in prompts_with_dependencies:\n",
    "        text = f\"\"\"\n",
    "        <|assistant|>\n",
    "        {_prompt}\n",
    "        Summary:\n",
    "        {_summary}\n",
    "        <|assistant|>\n",
    "        {_title}:\n",
    "        {_text}<|endoftext|>\n",
    "        \"\"\"\n",
    "\n",
    "        return text\n",
    "\n",
    "    #######################\n",
    "    # Code-Text pair\n",
    "    #######################\n",
    "    else:\n",
    "        text = f\"\"\"\n",
    "        <|system|>\n",
    "        You are a helpful assistant.\n",
    "        {_prompt}\n",
    "        Code to analyze:\n",
    "        <|user|>\n",
    "        {_code}\n",
    "        <|assistant|>\n",
    "        {_title}:\n",
    "        {_text}<|endoftext|>\n",
    "        \"\"\"\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc0e7b1-0171-41f3-97a1-21fd1726d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PIPELINES\n",
    "##############################################################################\n",
    "def peft_finetuning_pipeline(dataset_name, use_dora=False):\n",
    "    \"\"\"\n",
    "    Executes the LoRA pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        [os.makedirs(dirname, exist_ok=True) for dirname in [\n",
    "            MODEL_DIR, EVAL_DIR\n",
    "        ]]\n",
    "    \n",
    "        ##############################################################################\n",
    "        # Early Stopping Callback\n",
    "        ##############################################################################\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            \n",
    "            early_stopping_threshold=0.001,\n",
    "        )   \n",
    "    \n",
    "        ##############################################################################\n",
    "        # Load models to finetune\n",
    "        ##############################################################################\n",
    "        for model_id in MODEL_IDS:\n",
    "    \n",
    "            print(f\"Start finetuning {model_id}...\")\n",
    "\n",
    "            base_model_dir = f\"{'dora' if use_dora else 'lora'}/{model_id.replace(\"/\",\"_\")}\"\n",
    "\n",
    "            [os.makedirs(dirname, exist_ok=True) for dirname in [\n",
    "                f\"{MODEL_DIR}/{base_model_dir}/experiment\",\n",
    "                f\"{MODEL_DIR}/{base_model_dir}/final\",\n",
    "                f\"{MODEL_DIR}/{base_model_dir}/evals\",\n",
    "                f\"{MODEL_DIR}/{base_model_dir}/model\",\n",
    "            ]]\n",
    "    \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                \n",
    "                model_id,\n",
    "                \n",
    "                device_map=\"auto\",\n",
    "\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "    \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "            if tokenizer.pad_token is None:\n",
    "                \n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            tokenizer.padding_side = 'right'\n",
    "\n",
    "            train_dataset, test_dataset = build_datasets(dataset_name)\n",
    "        \n",
    "            ##############################################################################\n",
    "            # Data collator\n",
    "            ##############################################################################\n",
    "            collator = DataCollatorForLanguageModeling(\n",
    "                \n",
    "                tokenizer=tokenizer,\n",
    "                \n",
    "                mlm=False,\n",
    "            )\n",
    "\n",
    "            # response_template_ids = tokenizer.encode(\n",
    "                \n",
    "            #     \"\\n<|assistant|>\\n\", \n",
    "                \n",
    "            #     add_special_tokens=False\n",
    "            # )[2:]\n",
    "            \n",
    "            # collator = DataCollatorForCompletionOnlyLM(\n",
    "                \n",
    "            #     response_template_ids, \n",
    "                \n",
    "            #     tokenizer=tokenizer\n",
    "            # )\n",
    "\n",
    "            ##############################################################################\n",
    "            # Evaluation Metric Function\n",
    "            ##############################################################################\n",
    "            def compute_metrics(eval_preds: EvalPrediction):\n",
    "                \n",
    "                preds, labels = eval_preds\n",
    "                \n",
    "                decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "                \n",
    "                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "                \n",
    "                result = rouge_metric.compute(predictions=decoded_preds, \n",
    "                                              \n",
    "                                              references=decoded_labels, \n",
    "                                              \n",
    "                                              use_stemmer=True)\n",
    "                \n",
    "                result = {\"eval_\"+k: round(v * 100, 4) for k, v in result.items() if k.lower() in ['rougel']}\n",
    "            \n",
    "                return result\n",
    "            \n",
    "            ##############################################################################\n",
    "            # Objective Function for Hyperparameter Tuning\n",
    "            ##############################################################################\n",
    "            def objective(trial):\n",
    "\n",
    "                ##############################################################################\n",
    "                # Hyperparameters\n",
    "                ##############################################################################\n",
    "\n",
    "                learning_rate = trial.suggest_float(\n",
    "                    \"learning_rate\", 1e-5, 1e-4, log=True\n",
    "                )\n",
    "                \n",
    "                per_device_train_batch_size = trial.suggest_categorical(\n",
    "                    \"per_device_train_batch_size\", [1, 2]\n",
    "                )\n",
    "                \n",
    "                r = trial.suggest_categorical(\n",
    "                    \"r\", [8, 16, 32]\n",
    "                )\n",
    "                \n",
    "                lora_alpha = trial.suggest_categorical(\n",
    "                    \"lora_alpha\", [16, 32, 64]\n",
    "                )\n",
    "                \n",
    "                lora_dropout = trial.suggest_categorical(\n",
    "                    \"lora_dropout\", [0.05, 0.1]\n",
    "                )\n",
    "\n",
    "                num_train_epochs = trial.suggest_int(\n",
    "                    \"num_train_epochs\", 1, 3\n",
    "                )\n",
    "    \n",
    "                ##############################################################################\n",
    "                # LoRA / DORA Configuration\n",
    "                ##############################################################################\n",
    "            \n",
    "                lora_config = LoraConfig(\n",
    "                    r=r, \n",
    "                    \n",
    "                    lora_alpha=lora_alpha,\n",
    "                    \n",
    "                    target_modules=['q_proj', 'k_proj', 'v_proj'],\n",
    "                    \n",
    "                    lora_dropout=lora_dropout,\n",
    "                    \n",
    "                    bias=\"none\",\n",
    "            \n",
    "                    use_dora=use_dora,\n",
    "                )\n",
    "\n",
    "                ##############################################################################\n",
    "                # Training Arguments / SFTConfig\n",
    "                ##############################################################################\n",
    "                training_args = SFTConfig(\n",
    "                    \n",
    "                    output_dir=f\"{MODEL_DIR}/{base_model_dir}/experiment\",\n",
    "                    \n",
    "                    learning_rate=learning_rate,\n",
    "                    \n",
    "                    per_device_train_batch_size=per_device_train_batch_size,\n",
    "                    \n",
    "                    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "                    \n",
    "                    num_train_epochs=num_train_epochs,\n",
    "                    \n",
    "                    logging_steps=100,\n",
    "                    \n",
    "                    fp16=True,\n",
    "                    \n",
    "                    report_to=\"none\",\n",
    "                    \n",
    "                    eval_strategy=\"epoch\",  \n",
    "                    \n",
    "                    save_strategy=\"epoch\",   \n",
    "                    \n",
    "                    load_best_model_at_end=True,  \n",
    "                \n",
    "                    metric_for_best_model=\"eval_loss\", \n",
    "                    \n",
    "                    greater_is_better=False,   \n",
    "                    \n",
    "                    max_length=8192,\n",
    "                    \n",
    "                    packing=False,    \n",
    "                \n",
    "                    seed=42,\n",
    "                )\n",
    "        \n",
    "                ##############################################################################\n",
    "                # Supervised Finetuning Trainer\n",
    "                ##############################################################################\n",
    "            \n",
    "                trainer = SFTTrainer(\n",
    "                    \n",
    "                    model=get_peft_model(\n",
    "                    \n",
    "                        model, \n",
    "                        \n",
    "                        lora_config\n",
    "                    ),\n",
    "                    \n",
    "                    args=training_args,\n",
    "                    \n",
    "                    train_dataset=train_dataset,\n",
    "                    \n",
    "                    eval_dataset=test_dataset,\n",
    "                    \n",
    "                    peft_config = lora_config,\n",
    "                    \n",
    "                    formatting_func = code_text_formatter,\n",
    "                    \n",
    "                    data_collator = collator,\n",
    "                    \n",
    "                    callbacks=[early_stopping_callback],\n",
    "                )\n",
    "\n",
    "                trainer.train()\n",
    "\n",
    "                return trainer.state.best_metric\n",
    "                \n",
    "\n",
    "            ##############################################################################\n",
    "            # Perform Hyperparameter Search\n",
    "            ##############################################################################\\\n",
    "\n",
    "            study = optuna.create_study(direction=\"minimize\") # Minimize loss\n",
    "            \n",
    "            study.optimize(objective, n_trials=10)\n",
    "    \n",
    "            final_lora_config = LoraConfig(\n",
    "                r=study.best_params[\"r\"], \n",
    "                \n",
    "                lora_alpha=study.best_params[\"lora_alpha\"],\n",
    "                \n",
    "                target_modules=['q_proj', 'k_proj', 'v_proj'],\n",
    "                \n",
    "                lora_dropout=study.best_params[\"lora_dropout\"],\n",
    "                \n",
    "                bias=\"none\",\n",
    "        \n",
    "                use_dora=use_dora,\n",
    "            )\n",
    "            \n",
    "            final_training_args = SFTConfig(\n",
    "                output_dir=f\"{MODEL_DIR}/{base_model_dir}/final\",\n",
    "            \n",
    "                learning_rate=study.best_params[\"learning_rate\"],\n",
    "                \n",
    "                per_device_train_batch_size=study.best_params[\"per_device_train_batch_size\"],\n",
    "                \n",
    "                per_device_eval_batch_size=study.best_params[\"per_device_train_batch_size\"],\n",
    "                \n",
    "                num_train_epochs=study.best_params[\"num_train_epochs\"],\n",
    "                \n",
    "                logging_steps=100,\n",
    "                \n",
    "                fp16=True,\n",
    "                \n",
    "                report_to=\"none\",\n",
    "                \n",
    "                eval_strategy=\"epoch\",  \n",
    "                \n",
    "                save_strategy=\"epoch\",   \n",
    "                \n",
    "                load_best_model_at_end=True,  \n",
    "            \n",
    "                metric_for_best_model=\"eval_loss\", \n",
    "                \n",
    "                greater_is_better=False,   \n",
    "                \n",
    "                max_length=8192,\n",
    "                \n",
    "                packing=False,    \n",
    "            \n",
    "                seed=42,\n",
    "            )\n",
    "            \n",
    "            final_trainer = SFTTrainer(\n",
    "                \n",
    "                model=get_peft_model(\n",
    "                    \n",
    "                    model, \n",
    "                    \n",
    "                    final_lora_config\n",
    "                ),\n",
    "                \n",
    "                args=final_training_args,\n",
    "                \n",
    "                train_dataset=train_dataset,\n",
    "                \n",
    "                eval_dataset=test_dataset,\n",
    "                \n",
    "                peft_config = final_lora_config,\n",
    "                \n",
    "                formatting_func = code_text_formatter,\n",
    "                \n",
    "                data_collator = collator,\n",
    "                \n",
    "                callbacks=[early_stopping_callback],\n",
    "            )\n",
    "            \n",
    "            ##############################################################################\n",
    "            # Start finetuning!\n",
    "            ##############################################################################\n",
    "            final_trainer.train()\n",
    "\n",
    "            ##############################################################################\n",
    "            # Capture metrics\n",
    "            ##############################################################################\n",
    "\n",
    "            log_history = final_trainer.state.log_history\n",
    "\n",
    "            current_date = datetime.now().strftime('%Y%m%d%H%M')\n",
    "\n",
    "            final_metrics_file_name = f\"{MODEL_DIR}/{base_model_dir}/evals/finetune_{current_date}.txt\"\n",
    "\n",
    "            with open(final_metrics_file_name, \"a\") as f:\n",
    "\n",
    "                json.dump(log_history, f)\n",
    "    \n",
    "            ##############################################################################\n",
    "            # Save snapshot and push to HuggingFace Hub\n",
    "            ##############################################################################\n",
    "\n",
    "            try:\n",
    "            \n",
    "                model.save_pretrained(f\"{MODEL_DIR}/{base_model_dir}/model\")\n",
    "                \n",
    "                tokenizer.save_pretrained(f\"{MODEL_DIR}/{base_model_dir}/model\")\n",
    "\n",
    "                published_model_id = f\"{model_id.partition(\"/\")[2] or model_id}_l\" # For LoRA variant\n",
    "\n",
    "                model.push_to_hub(published_model_id)\n",
    "\n",
    "                tokenizer.push_to_hub(published_model_id)\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"Error saving and pushing to HuggingFace: {e}\")\n",
    "        \n",
    "                traceback.print_exc()\n",
    "            \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error running PEFT pipeline: {e}\")\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "def lora_finetuning_pipeline(dataset_name):\n",
    "    \"\"\"\n",
    "    Executes the LoRA pipeline.\n",
    "    \"\"\"\n",
    "    return peft_finetuning_pipeline(dataset_name, use_dora=False)\n",
    "        \n",
    "def dora_finetuning_pipeline(dataset_name):\n",
    "    \"\"\"\n",
    "    Executes the DORA pipeline.\n",
    "    \"\"\"\n",
    "    return peft_finetuning_pipeline(dataset_name, use_dora=True)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c31ca-9551-46d8-807b-64284e9f1cbd",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "Execute the pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5148e-7f72-402f-a194-7655fd4e5a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-4.0-h-tiny...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d8ccb59db04e9babaf0bbb272941a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 14:14:45,297] A new study created in memory with name: no-name-c2e0e1a1-8b14-490c-93ad-f98c8ca2d346\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2496' max='2496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2496/2496 1:04:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.649724</td>\n",
       "      <td>0.606538</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.862185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.601545</td>\n",
       "      <td>0.559368</td>\n",
       "      <td>794510.000000</td>\n",
       "      <td>0.870970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 15:19:16,447] Trial 0 finished with value: 0.601544976234436 and parameters: {'learning_rate': 2.5530267659850096e-05, 'per_device_train_batch_size': 1, 'r': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'num_train_epochs': 2}. Best is trial 0 with value: 0.601544976234436.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1248' max='1248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1248/1248 31:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.608314</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.871000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 15:50:29,201] Trial 1 finished with value: 0.6083135604858398 and parameters: {'learning_rate': 3.562216594300751e-05, 'per_device_train_batch_size': 1, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.05, 'num_train_epochs': 1}. Best is trial 0 with value: 0.601544976234436.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3744' max='3744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3744/3744 1:31:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>0.482891</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.880589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.482027</td>\n",
       "      <td>0.464616</td>\n",
       "      <td>794510.000000</td>\n",
       "      <td>0.883157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>0.477636</td>\n",
       "      <td>0.454831</td>\n",
       "      <td>1191765.000000</td>\n",
       "      <td>0.883253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 17:22:29,717] Trial 2 finished with value: 0.4776360094547272 and parameters: {'learning_rate': 4.824352506624695e-05, 'per_device_train_batch_size': 1, 'r': 16, 'lora_alpha': 64, 'lora_dropout': 0.05, 'num_train_epochs': 3}. Best is trial 2 with value: 0.4776360094547272.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3744' max='3744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3744/3744 1:32:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.561600</td>\n",
       "      <td>0.569379</td>\n",
       "      <td>0.565431</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.871367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.494800</td>\n",
       "      <td>0.528149</td>\n",
       "      <td>0.504335</td>\n",
       "      <td>794510.000000</td>\n",
       "      <td>0.878222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.521593</td>\n",
       "      <td>0.499215</td>\n",
       "      <td>1191765.000000</td>\n",
       "      <td>0.879721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 18:55:01,155] Trial 3 finished with value: 0.5215930342674255 and parameters: {'learning_rate': 1.8108380375566948e-05, 'per_device_train_batch_size': 1, 'r': 16, 'lora_alpha': 64, 'lora_dropout': 0.1, 'num_train_epochs': 3}. Best is trial 2 with value: 0.4776360094547272.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3744' max='3744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3744/3744 1:34:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.575361</td>\n",
       "      <td>0.541490</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.872296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.523616</td>\n",
       "      <td>0.499394</td>\n",
       "      <td>794510.000000</td>\n",
       "      <td>0.879071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.465200</td>\n",
       "      <td>0.518331</td>\n",
       "      <td>0.492620</td>\n",
       "      <td>1191765.000000</td>\n",
       "      <td>0.880173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 20:29:26,680] Trial 4 finished with value: 0.5183305144309998 and parameters: {'learning_rate': 4.174920902491335e-05, 'per_device_train_batch_size': 1, 'r': 32, 'lora_alpha': 16, 'lora_dropout': 0.05, 'num_train_epochs': 3}. Best is trial 2 with value: 0.4776360094547272.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1872' max='1872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1872/1872 1:42:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.649800</td>\n",
       "      <td>0.636538</td>\n",
       "      <td>0.615453</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.861380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.557773</td>\n",
       "      <td>0.552305</td>\n",
       "      <td>794510.000000</td>\n",
       "      <td>0.873241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>0.539806</td>\n",
       "      <td>0.527134</td>\n",
       "      <td>1191765.000000</td>\n",
       "      <td>0.877492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 22:12:27,504] Trial 5 finished with value: 0.5398059487342834 and parameters: {'learning_rate': 1.8607032895241005e-05, 'per_device_train_batch_size': 2, 'r': 16, 'lora_alpha': 64, 'lora_dropout': 0.05, 'num_train_epochs': 3}. Best is trial 2 with value: 0.4776360094547272.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [624/624 33:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.135800</td>\n",
       "      <td>1.150564</td>\n",
       "      <td>1.111728</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.744602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 22:46:01,048] Trial 6 finished with value: 1.1505643129348755 and parameters: {'learning_rate': 1.4230427220504285e-05, 'per_device_train_batch_size': 2, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1, 'num_train_epochs': 1}. Best is trial 2 with value: 0.4776360094547272.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2496' max='2496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2496/2496 1:03:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>0.742625</td>\n",
       "      <td>0.687557</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.840562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.656600</td>\n",
       "      <td>0.670763</td>\n",
       "      <td>0.626205</td>\n",
       "      <td>794510.000000</td>\n",
       "      <td>0.859236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 23:49:08,953] Trial 7 finished with value: 0.6707632541656494 and parameters: {'learning_rate': 1.7006660943107813e-05, 'per_device_train_batch_size': 1, 'r': 16, 'lora_alpha': 16, 'lora_dropout': 0.05, 'num_train_epochs': 2}. Best is trial 2 with value: 0.4776360094547272.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1872' max='1872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1872/1872 1:40:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.789584</td>\n",
       "      <td>0.739401</td>\n",
       "      <td>397255.000000</td>\n",
       "      <td>0.832102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>0.639841</td>\n",
       "      <td>0.604394</td>\n",
       "      <td>794510.000000</td>\n",
       "      <td>0.864894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.591600</td>\n",
       "      <td>0.620465</td>\n",
       "      <td>0.589620</td>\n",
       "      <td>1191765.000000</td>\n",
       "      <td>0.868371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-04 01:29:20,361] Trial 8 finished with value: 0.620465099811554 and parameters: {'learning_rate': 1.4282507361820906e-05, 'per_device_train_batch_size': 2, 'r': 32, 'lora_alpha': 32, 'lora_dropout': 0.05, 'num_train_epochs': 3}. Best is trial 2 with value: 0.4776360094547272.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='1872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 164/1872 08:15 < 1:27:00, 0.33 it/s, Epoch 0.26/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dora_finetuning_pipeline(f\"{os.getenv('HF_USERNAME')}/jsp-code-to-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11918bcd-0bd3-49ff-a357-ed59a6388cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trainer.state.best_model_checkpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
