{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f44884d-38c7-4656-a45e-b1b96afe7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Handle imports\n",
    "##############################################################################\n",
    "from docling.document_converter import DocumentConverter\n",
    "import traceback\n",
    "from collections.abc import Iterable\n",
    "import os\n",
    "import pypdfium2 as pdfium\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import uuid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval, AnswerRelevancyMetric\n",
    "from deepeval import evaluate\n",
    "from huggingface_hub import snapshot_download, login, HfApi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import xml.etree.ElementTree as etree\n",
    "from longdocfactscore.ldfacts import LongDocFACTScore\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "nltk.download('punkt_tab')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import EarlyStoppingCallback\n",
    "import torch\n",
    "import optuna\n",
    "import traceback\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2df5d2-b6ab-41a2-bdcc-398fd85c45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Set up variables\n",
    "##############################################################################\n",
    "SOURCE_DIR=\"source_docs\"\n",
    "SOURCE_DIR_CHUNKED=\"source_docs_chunked\"\n",
    "MARKDOWN_DIR=\"markdown\"\n",
    "MARKDOWN_URI_PREFIX=\"https://raw.githubusercontent.com/agapebondservant/code-generation-capstone/refs/heads/main/eda/resources\"\n",
    "REPORT_DIR=\"reports\"\n",
    "OUTPUT_DIR=\"output\"\n",
    "INVALID_DIR=\"invalid\"\n",
    "ERROR_DIR=\"error\" \n",
    "MODEL_DIR=\"models\"\n",
    "MODEL_IDS = [\"ibm-granite/granite-8b-code-instruct-4k\",\"ibm-granite/granite-8b-code-base-128k\"]\n",
    "DEVICE=\"cuda\"\n",
    "DATASET_REPO=f\"{os.getenv('HF_USERNAME')}/codegen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146bed40-2949-4e17-8bc1-745b44393567",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Set up object instances\n",
    "##############################################################################\n",
    "\n",
    "data_generator_llm = ChatOpenAI(\n",
    "    model=os.getenv(\"DATA_GENERATOR_MODEL_ID\"), # os.getenv('QWEN25CODER_MODEL_ID'),\n",
    "    api_key=os.getenv('OPENROUTER_TOKEN'),\n",
    "    base_url=os.getenv('OPENROUTER_API_BASE'),\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "class DataGeneratorLLM(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Data Generator LLM (GPT-OSS)\"\n",
    "\n",
    "evaluator_llm = DataGeneratorLLM(data_generator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa50c10-3803-4a75-b143-4efc12f75988",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PROMPTS AND PROMPT TYPES\n",
    "##############################################################################\n",
    "\n",
    "summary_prompt = \"\"\"\n",
    "Your task is to analyze this code snippet and provide an explanation of the code.\n",
    "    \n",
    "Instructions:\n",
    "1. Provide a concise explanation that summarizes the purpose of the code without getting into too many specific technical details.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "3. Also exclude any details that link the requirements to a specific programming language or framework.\n",
    "\"\"\"\n",
    "\n",
    "topics_prompt = \"\"\"\n",
    "Use the provided summary to analyze this code snippet and generate a list of programming topics that are related to the code.\n",
    "    \n",
    "Instructions:\n",
    "1. Provide a short list of topics that you can identify.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "components_prompt = \"\"\"\n",
    "Your task is to analyze this code snippet and generate a specification of all the JSP relevant components you can find.\n",
    "\n",
    "Instructions:\n",
    "1. Include only relevant components.\n",
    "3. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "domain_prompt = \"\"\"\n",
    "Your task is to analyze this code snippet and generate an outline of the domain model associated with this code.\n",
    "    \n",
    "Instructions:\n",
    "1. Avoid getting into too many specific technical details. Simply provide a domain model of the code.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "3. Include the current state of the domain objects based on information extracted from the code.\n",
    "\"\"\"\n",
    "\n",
    "keywords_prompt = \"\"\"\n",
    "Your task is to analyze this code snippet and generate a list of keywords that are associated with the code.\n",
    "    \n",
    "Instructions:\n",
    "1. Provide a short list of keywords.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "functional_requirements_prompt = \"\"\"\n",
    "Use the provided summary to analyze this code snippet and generate a list of programming topics that are related to the code.\n",
    "    \n",
    "Instructions:\n",
    "1. Provide a short list of topics that you can identify.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "business_requirements_prompt = \"\"\"\n",
    "Use the provided summary to generate an outline of sample business requirements that might be connected to the code.\n",
    "\n",
    "Instructions:\n",
    "1. Provide a short list of relevant requirements. Do not include requirements that are not related to the code.\n",
    "2. If the provided snippet does not appear to be a code snippet, indicate that this is not valid code.\n",
    "\"\"\"\n",
    "\n",
    "prompts = {\n",
    "\n",
    "    \"functional_requirements\": {\n",
    "        \n",
    "        \"prompt\": functional_requirements_prompt, \n",
    "\n",
    "        \"title\": \"Functional Requirements\",\n",
    "    },\n",
    "    \"business_requirements\": {\n",
    "        \n",
    "        \"prompt\": business_requirements_prompt, \n",
    "\n",
    "        \"title\": \"Business Requirements\",\n",
    "    },\n",
    "    \"topics\": {\n",
    "        \n",
    "        \"prompt\": topics_prompt, \n",
    "\n",
    "        \"title\": \"Components\",\n",
    "    },\n",
    "    \"components\": {\n",
    "        \n",
    "        \"prompt\": components_prompt,\n",
    "\n",
    "        \"title\": \"Topics\",\n",
    "    },\n",
    "    \"keywords\": {\n",
    "        \n",
    "        \"prompt\": keywords_prompt,\n",
    "\n",
    "        \"title\": \"Keywords\",\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \n",
    "        \"prompt\": summary_prompt,\n",
    "\n",
    "        \"title\": \"Summary\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "prompts_with_dependencies = {\n",
    "    \"topics\": \"summary\",\n",
    "    \n",
    "    \"business_requirements\": \"summary\",\n",
    "    \n",
    "    \"functional_requirements\": \"summary\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a16f1a-8c36-4e70-b4f6-fb7ac24e54b0",
   "metadata": {},
   "source": [
    "### Download candidate models\n",
    "The following candidate models will be downloaded:\n",
    "- ibm-granite/granite-8b-code-instruct-4k\n",
    "- ibm-granite/granite-8b-code-base-128k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee5b701-d7ff-404c-87e4-c0893efd1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# UTILITY METHODS\n",
    "##############################################################################\n",
    "\n",
    "def download_models(repo_id):\n",
    "    try:\n",
    "        ##############################################################################\n",
    "        # Save the model\n",
    "        ##############################################################################\n",
    "        local_dir = snapshot_download(repo_id=repo_id, cache_dir=MODEL_DIR)\n",
    "        \n",
    "        print(f\"Model {repo_id} downloaded to: {local_dir}\")\n",
    "\n",
    "        ##############################################################################\n",
    "        # Save the tokenizer\n",
    "        ##############################################################################\n",
    "        tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            \n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        tokenizer.save_pretrained(local_dir)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "    \n",
    "        print(f\"Error downloading model {repo_id}: {e}\")\n",
    "\n",
    "def upload_models(repo_id, model_dir):\n",
    "\n",
    "    try:\n",
    "    \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_dir, \n",
    "                                                     trust_remote_code=True,\n",
    "                                                     device_map=DEVICE)\n",
    "    \n",
    "        api = HfApi()\n",
    "    \n",
    "        api.create_repo(repo_id=repo_id, repo_type=\"model\")\n",
    "    \n",
    "        api.upload_folder(\n",
    "            folder_path=model_dir,\n",
    "            \n",
    "            repo_id=repo_id,\n",
    "            \n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "    \n",
    "        print(f\"Error uploading model {repo_id} from directory {model_dir}: {e}\")\n",
    "\n",
    "def build_datasets(dataset_name):\n",
    "\n",
    "    final_datasets = []\n",
    "\n",
    "    def process_summary_to_text(example, code_type=\"\"):\n",
    "        \n",
    "        example[\"text\"], example[\"completion\"], example[\"code_type\"] = example[\"summary\"], example[code_type], [code_type]*len(example[\"code\"])\n",
    "        \n",
    "        return example\n",
    "\n",
    "    def process_code_to_text(example, code_type=\"\"):\n",
    "        example[\"text\"], example[\"completion\"], example[\"code_type\"] = example[\"code\"], example[code_type], [code_type]*len(example[\"code\"])\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    test_dataset = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "    for dataset in [train_dataset, test_dataset]:\n",
    "\n",
    "        datasets = []\n",
    "\n",
    "        code_types = [c for c, obj in prompts.items() if c not in ['code']]\n",
    "        \n",
    "        for code_type in code_types:\n",
    "\n",
    "            if code_type in prompts_with_dependencies:\n",
    "\n",
    "                datasets.append(dataset.map(process_summary_to_text, batched=True, fn_kwargs={\"code_type\": code_type}))\n",
    "            \n",
    "            else:\n",
    "\n",
    "                datasets.append(dataset.map(process_code_to_text, batched=True, fn_kwargs={\"code_type\": code_type}))\n",
    "\n",
    "        final_datasets.append(interleave_datasets(datasets))\n",
    "\n",
    "    return final_datasets\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d90482-ea95-4402-86c6-02a51d1bf92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Code Formatting Helper Function\n",
    "##############################################################################\n",
    "def code_text_formatter(example):\n",
    "\n",
    "    _code = example['code']\n",
    "    \n",
    "    _summary = example['summary']\n",
    "\n",
    "    _code_type = example[\"code_type\"]\n",
    "\n",
    "    _text = example['text']\n",
    "\n",
    "    _prompt = prompts[_code_type][\"prompt\"]\n",
    "\n",
    "    _title = prompts[_code_type][\"title\"]\n",
    "\n",
    "    ######################################\n",
    "    # Code-Summary pair\n",
    "    ######################################\n",
    "    if _code_type in prompts_with_dependencies:\n",
    "        text = f\"\"\"\n",
    "        <|assistant|>\n",
    "        {_prompt}\n",
    "        Summary:\n",
    "        {_summary}\n",
    "        <|assistant|>\n",
    "        {_title}:\n",
    "        {_text}<|endoftext|>\n",
    "        \"\"\"\n",
    "\n",
    "        return text\n",
    "\n",
    "    #######################\n",
    "    # Code-Text pair\n",
    "    #######################\n",
    "    else:\n",
    "        text = f\"\"\"\n",
    "        <|system|>\n",
    "        You are a helpful assistant.\n",
    "        {_prompt}\n",
    "        Code to analyze:\n",
    "        <|user|>\n",
    "        {_code}\n",
    "        <|assistant|>\n",
    "        {_title}:\n",
    "        {_text}<|endoftext|>\n",
    "        \"\"\"\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc0e7b1-0171-41f3-97a1-21fd1726d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# PIPELINES\n",
    "##############################################################################\n",
    "def peft_finetuning_pipeline(dataset_name, use_dora=False):\n",
    "    \"\"\"\n",
    "    Executes the LoRA pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        [os.makedirs(dirname, exist_ok=True) for dirname in [\n",
    "            MODEL_DIR\n",
    "        ]]\n",
    "    \n",
    "        ##############################################################################\n",
    "        # Early Stopping Callback\n",
    "        ##############################################################################\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            \n",
    "            early_stopping_threshold=0.001,\n",
    "        )   \n",
    "    \n",
    "        ##############################################################################\n",
    "        # Load models to finetune\n",
    "        ##############################################################################\n",
    "        for model_id in MODEL_IDS:\n",
    "    \n",
    "            print(f\"Start finetuning {model_id}...\")\n",
    "\n",
    "            base_model_dir = model_id.replace(\"/\",\"_\")\n",
    "\n",
    "            [os.makedirs(dirname, exist_ok=True) for dirname in [\n",
    "                MODEL_DIR/base_model_dir/\"experiment\",\n",
    "                MODEL_DIR/base_model_dir/\"final\",\n",
    "                MODEL_DIR/base_model_dir/\"model\",\n",
    "            ]]\n",
    "    \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                \n",
    "                model_id,\n",
    "                \n",
    "                device_map=\"auto\",\n",
    "\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "    \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "            if tokenizer.pad_token is None:\n",
    "                \n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            train_dataset, test_dataset = build_datasets(dataset_name)\n",
    "        \n",
    "            ##############################################################################\n",
    "            # Data collator\n",
    "            ##############################################################################\n",
    "            collator = DataCollatorForLanguageModeling(\n",
    "                \n",
    "                tokenizer=tokenizer,\n",
    "                \n",
    "                mlm=False,\n",
    "            )\n",
    "            \n",
    "            ##############################################################################\n",
    "            # Objective Function for Hyperparameter Tuning\n",
    "            ##############################################################################\n",
    "            def objective(trial):\n",
    "\n",
    "                ##############################################################################\n",
    "                # Hyperparameters\n",
    "                ##############################################################################\n",
    "\n",
    "                learning_rate = trial.suggest_float(\n",
    "                    \"learning_rate\", 1e-5, 1e-4, log=True\n",
    "                )\n",
    "                \n",
    "                per_device_train_batch_size = trial.suggest_categorical(\n",
    "                    \"per_device_train_batch_size\", [16, 32]\n",
    "                )\n",
    "                \n",
    "                r = trial.suggest_categorical(\n",
    "                    \"r\", [8, 16, 32]\n",
    "                )\n",
    "                \n",
    "                lora_alpha = trial.suggest_categorical(\n",
    "                    \"lora_alpha\", [16, 32, 64]\n",
    "                )\n",
    "                \n",
    "                lora_dropout = trial.suggest_categorical(\n",
    "                    \"lora_dropout\", [0.05, 0.1]\n",
    "                )\n",
    "                \n",
    "    \n",
    "                ##############################################################################\n",
    "                # LoRA / DORA Configuration\n",
    "                ##############################################################################\n",
    "            \n",
    "                lora_config = LoraConfig(\n",
    "                    r=r, \n",
    "                    \n",
    "                    lora_alpha=lora_alpha,\n",
    "                    \n",
    "                    target_modules=None,\n",
    "                    \n",
    "                    lora_dropout=lora_dropout,\n",
    "                    \n",
    "                    bias=\"none\",\n",
    "            \n",
    "                    use_dora=use_dora,\n",
    "                )\n",
    "\n",
    "                ##############################################################################\n",
    "                # Training Arguments / SFTConfig\n",
    "                ##############################################################################\n",
    "                training_args = SFTConfig(\n",
    "                    \n",
    "                    output_dir=MODEL_DIR/base_model_dir/\"experiment\",\n",
    "                    \n",
    "                    learning_rate=learning_rate,\n",
    "                    \n",
    "                    per_device_train_batch_size=per_device_train_batch_size,\n",
    "                    \n",
    "                    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "                    \n",
    "                    num_train_epochs=10,\n",
    "                    \n",
    "                    logging_steps=100,\n",
    "                    \n",
    "                    fp16=True,\n",
    "                    \n",
    "                    report_to=\"none\",\n",
    "                    \n",
    "                    eval_strategy=\"epoch\",  \n",
    "                    \n",
    "                    save_strategy=\"epoch\",   \n",
    "                    \n",
    "                    load_best_model_at_end=True,  \n",
    "                \n",
    "                    metric_for_best_model=\"eval_loss\", \n",
    "                    \n",
    "                    greater_is_better=False,   \n",
    "                    \n",
    "                    max_length=4096,\n",
    "                    \n",
    "                    packing=False,    \n",
    "                \n",
    "                    seed=42,\n",
    "                )\n",
    "        \n",
    "                ##############################################################################\n",
    "                # Supervised Finetuning Trainer\n",
    "                ##############################################################################\n",
    "            \n",
    "                trainer = SFTTrainer(\n",
    "                    \n",
    "                    model=model,\n",
    "                    \n",
    "                    args=training_args,\n",
    "                    \n",
    "                    train_dataset=train_dataset,\n",
    "                    \n",
    "                    eval_dataset=test_dataset,\n",
    "                    \n",
    "                    peft_config = lora_config,\n",
    "                    \n",
    "                    formatting_func = code_text_formatter,\n",
    "                    \n",
    "                    data_collator = collator,\n",
    "                    \n",
    "                    callbacks=[early_stopping_callback],\n",
    "                )\n",
    "\n",
    "                trainer.train()\n",
    "\n",
    "                return trainer.state.best_metric\n",
    "                \n",
    "\n",
    "            ##############################################################################\n",
    "            # Perform Hyperparameter Search\n",
    "            ##############################################################################\\\n",
    "\n",
    "            study = optuna.create_study(direction=\"minimize\") # Minimize loss\n",
    "            \n",
    "            study.optimize(objective, n_trials=10)\n",
    "    \n",
    "            final_lora_config = LoraConfig(\n",
    "                r=study.best_params[\"r\"], \n",
    "                \n",
    "                lora_alpha=study.best_params[\"lora_alpha\"],\n",
    "                \n",
    "                target_modules=None,\n",
    "                \n",
    "                lora_dropout=study.best_params[\"lora_dropout\"],\n",
    "                \n",
    "                bias=\"none\",\n",
    "        \n",
    "                use_dora=use_dora,\n",
    "            )\n",
    "            \n",
    "            final_training_args = SFTConfig(\n",
    "                output_dir=MODEL_DIR/base_model_dir/\"final\",\n",
    "            \n",
    "                learning_rate=study.best_params[\"learning_rate\"],\n",
    "                \n",
    "                per_device_train_batch_size=study.best_params[\"per_device_train_batch_size\"],\n",
    "                \n",
    "                per_device_eval_batch_size=study.best_params[\"per_device_train_batch_size\"],\n",
    "                \n",
    "                num_train_epochs=10,\n",
    "                \n",
    "                logging_steps=100,\n",
    "                \n",
    "                fp16=True,\n",
    "                \n",
    "                report_to=\"none\",\n",
    "                \n",
    "                eval_strategy=\"epoch\",  \n",
    "                \n",
    "                save_strategy=\"epoch\",   \n",
    "                \n",
    "                load_best_model_at_end=True,  \n",
    "            \n",
    "                metric_for_best_model=\"eval_loss\", \n",
    "                \n",
    "                greater_is_better=False,   \n",
    "                \n",
    "                max_length=4096,\n",
    "                \n",
    "                packing=False,    \n",
    "            \n",
    "                seed=42,\n",
    "            )\n",
    "            \n",
    "            final_trainer = SFTTrainer(\n",
    "                \n",
    "                model=model,\n",
    "                \n",
    "                args=final_training_args,\n",
    "                \n",
    "                train_dataset=train_dataset,\n",
    "                \n",
    "                eval_dataset=test_dataset,\n",
    "                \n",
    "                peft_config = final_lora_config,\n",
    "                \n",
    "                formatting_func = code_text_formatter,\n",
    "                \n",
    "                data_collator = collator,\n",
    "                \n",
    "                callbacks=[early_stopping_callback],\n",
    "            )\n",
    "            \n",
    "            ##############################################################################\n",
    "            # Start finetuning!\n",
    "            ##############################################################################\n",
    "            final_trainer.train()\n",
    "    \n",
    "            ##############################################################################\n",
    "            # Save snapshot and push to HuggingFace Hub\n",
    "            ##############################################################################\n",
    "\n",
    "            try:\n",
    "            \n",
    "                model.save_pretrained(MODEL_DIR/base_model_dir/\"model\")\n",
    "                \n",
    "                tokenizer.save_pretrained(MODEL_DIR/base_model_dir/\"model\")\n",
    "\n",
    "                published_model_id = model_id.partition(\"/\")[2] or model_id\n",
    "\n",
    "                model.push_to_hub(published_model_id)\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"Error saving and pushing to HuggingFace: {e}\")\n",
    "        \n",
    "                traceback.print_exc()\n",
    "            \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error running PEFT pipeline: {e}\")\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "def lora_finetuning_pipeline(dataset_name):\n",
    "    \"\"\"\n",
    "    Executes the LoRA pipeline.\n",
    "    \"\"\"\n",
    "    return peft_finetuning_pipeline(dataset_name, use_dora=False)\n",
    "        \n",
    "def dora_finetuning_pipeline(dataset_name):\n",
    "    \"\"\"\n",
    "    Executes the DORA pipeline.\n",
    "    \"\"\"\n",
    "    return peft_finetuning_pipeline(dataset_name, use_dora=True)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c31ca-9551-46d8-807b-64284e9f1cbd",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "Execute the pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b5148e-7f72-402f-a194-7655fd4e5a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 05:16:03,248] A new study created in memory with name: no-name-b20b472b-84d4-4cde-baca-6bbc3a56a7f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 05:16:03,899 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e9b17d24ec4daaa73df3c723faa27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 25:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.667576</td>\n",
       "      <td>0.638615</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.855645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.566938</td>\n",
       "      <td>0.530779</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.873005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.766500</td>\n",
       "      <td>0.539024</td>\n",
       "      <td>0.491951</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.876737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.766500</td>\n",
       "      <td>0.515617</td>\n",
       "      <td>0.462160</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.880178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.766500</td>\n",
       "      <td>0.500406</td>\n",
       "      <td>0.446687</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.881825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.488214</td>\n",
       "      <td>0.423459</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.886007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.485286</td>\n",
       "      <td>0.398853</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.885761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.494797</td>\n",
       "      <td>0.369039</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.886097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.497241</td>\n",
       "      <td>0.361290</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.885624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.498280</td>\n",
       "      <td>0.359393</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.885552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 05:41:37,046] Trial 0 finished with value: 0.4852861762046814 and parameters: {'learning_rate': 9.559733394045224e-05, 'per_device_train_batch_size': 32, 'r': 8, 'lora_alpha': 64, 'lora_dropout': 0.05}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 05:41:37,172 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241a24c1637947b3884c0d20b99c1d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 22:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.699664</td>\n",
       "      <td>0.671339</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.848620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.044300</td>\n",
       "      <td>0.575546</td>\n",
       "      <td>0.538176</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.872612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.562200</td>\n",
       "      <td>0.550816</td>\n",
       "      <td>0.505889</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.875699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>0.534113</td>\n",
       "      <td>0.486414</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.878801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>0.522573</td>\n",
       "      <td>0.475035</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.880047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.485200</td>\n",
       "      <td>0.511664</td>\n",
       "      <td>0.461564</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.881748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.501671</td>\n",
       "      <td>0.452760</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.882547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.436200</td>\n",
       "      <td>0.497040</td>\n",
       "      <td>0.442574</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.882917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.494651</td>\n",
       "      <td>0.438579</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.883454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.494482</td>\n",
       "      <td>0.434681</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.883118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 06:03:55,236] Trial 1 finished with value: 0.4944818913936615 and parameters: {'learning_rate': 5.151115763529888e-05, 'per_device_train_batch_size': 16, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 06:03:55,361 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8886bd07fd441cb15aea8ca2bae99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 25:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.030858</td>\n",
       "      <td>1.023342</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.771341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.634351</td>\n",
       "      <td>0.601590</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.861545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>0.583014</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>0.566059</td>\n",
       "      <td>0.525362</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.872855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>0.552976</td>\n",
       "      <td>0.515040</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.874947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.545100</td>\n",
       "      <td>0.544287</td>\n",
       "      <td>0.510028</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.876234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.545100</td>\n",
       "      <td>0.538311</td>\n",
       "      <td>0.489178</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.877330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.534073</td>\n",
       "      <td>0.486135</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.877861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.878805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.531101</td>\n",
       "      <td>0.479228</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.878943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 06:29:30,042] Trial 2 finished with value: 0.531100869178772 and parameters: {'learning_rate': 6.546652626974007e-05, 'per_device_train_batch_size': 32, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 06:29:30,173 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91574015e68042de9330087e90ebe79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 25:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.453379</td>\n",
       "      <td>1.085384</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.711866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.345312</td>\n",
       "      <td>1.142937</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.718818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.467100</td>\n",
       "      <td>1.228181</td>\n",
       "      <td>1.086024</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.736601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.467100</td>\n",
       "      <td>1.123504</td>\n",
       "      <td>1.036935</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.753910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.467100</td>\n",
       "      <td>1.014178</td>\n",
       "      <td>0.905792</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.783732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.167300</td>\n",
       "      <td>0.917159</td>\n",
       "      <td>0.823116</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.803229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.167300</td>\n",
       "      <td>0.849851</td>\n",
       "      <td>0.772105</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.818287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.904200</td>\n",
       "      <td>0.806593</td>\n",
       "      <td>0.745216</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.828494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.904200</td>\n",
       "      <td>0.785022</td>\n",
       "      <td>0.729093</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.833400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.904200</td>\n",
       "      <td>0.778643</td>\n",
       "      <td>0.724313</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.834705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 06:55:03,298] Trial 3 finished with value: 0.7786428928375244 and parameters: {'learning_rate': 1.2118037018196061e-05, 'per_device_train_batch_size': 32, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.05}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 06:55:03,438 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0d2d880f6c446f9375441cac7c538f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 25:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.334222</td>\n",
       "      <td>1.158241</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.719743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.095506</td>\n",
       "      <td>1.039453</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.757898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.295500</td>\n",
       "      <td>0.823237</td>\n",
       "      <td>0.755867</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.823245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.295500</td>\n",
       "      <td>0.687408</td>\n",
       "      <td>0.655788</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.853127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.295500</td>\n",
       "      <td>0.634215</td>\n",
       "      <td>0.603073</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.863148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.724300</td>\n",
       "      <td>0.611048</td>\n",
       "      <td>0.579962</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.866435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.724300</td>\n",
       "      <td>0.600214</td>\n",
       "      <td>0.564550</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.869055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.594758</td>\n",
       "      <td>0.556287</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.869502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.591941</td>\n",
       "      <td>0.553450</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.870068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.591042</td>\n",
       "      <td>0.552307</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.870114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 07:20:38,598] Trial 4 finished with value: 0.5910419821739197 and parameters: {'learning_rate': 1.7558066996821136e-05, 'per_device_train_batch_size': 32, 'r': 16, 'lora_alpha': 64, 'lora_dropout': 0.1}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 07:20:38,744 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72569a5da2264c46b51403936e753b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 22:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.141581</td>\n",
       "      <td>1.084432</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.750288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.329700</td>\n",
       "      <td>0.682337</td>\n",
       "      <td>0.645384</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.855025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.728100</td>\n",
       "      <td>0.599664</td>\n",
       "      <td>0.562997</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.869394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.581541</td>\n",
       "      <td>0.539903</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.871513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.570469</td>\n",
       "      <td>0.534729</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.873305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.554100</td>\n",
       "      <td>0.562669</td>\n",
       "      <td>0.531792</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.873993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>0.556699</td>\n",
       "      <td>0.516076</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.874772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>0.552582</td>\n",
       "      <td>0.511524</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.875284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.550398</td>\n",
       "      <td>0.508923</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.875677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.549898</td>\n",
       "      <td>0.507392</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.875615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 07:42:56,950] Trial 5 finished with value: 0.5498979091644287 and parameters: {'learning_rate': 2.3556885553743287e-05, 'per_device_train_batch_size': 16, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 07:42:57,095 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c880d832295f43daa8b4b86b3f301327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 22:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.276755</td>\n",
       "      <td>1.135776</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.722978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.424400</td>\n",
       "      <td>0.872045</td>\n",
       "      <td>0.787426</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.815027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.660830</td>\n",
       "      <td>0.626535</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.858931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.646800</td>\n",
       "      <td>0.611721</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.867810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.646800</td>\n",
       "      <td>0.594391</td>\n",
       "      <td>0.556559</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.870358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.585300</td>\n",
       "      <td>0.586073</td>\n",
       "      <td>0.549078</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.871049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.570800</td>\n",
       "      <td>0.580714</td>\n",
       "      <td>0.541433</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.871953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.576675</td>\n",
       "      <td>0.536553</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.872410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.574526</td>\n",
       "      <td>0.535560</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.872762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.573885</td>\n",
       "      <td>0.534594</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.872583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 08:05:17,613] Trial 6 finished with value: 0.5738846063613892 and parameters: {'learning_rate': 2.3445691559115525e-05, 'per_device_train_batch_size': 16, 'r': 32, 'lora_alpha': 16, 'lora_dropout': 0.1}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 08:05:17,760 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c62a68875c940fa8d10e4b2cac084bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 22:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709852</td>\n",
       "      <td>0.679115</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.845326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.054200</td>\n",
       "      <td>0.577082</td>\n",
       "      <td>0.539214</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.872381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.564600</td>\n",
       "      <td>0.552416</td>\n",
       "      <td>0.507640</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.875252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.523700</td>\n",
       "      <td>0.535791</td>\n",
       "      <td>0.487625</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.878375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.523700</td>\n",
       "      <td>0.524446</td>\n",
       "      <td>0.477060</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.879874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.488300</td>\n",
       "      <td>0.515151</td>\n",
       "      <td>0.463950</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.881501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.464100</td>\n",
       "      <td>0.504501</td>\n",
       "      <td>0.455151</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.882152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.441800</td>\n",
       "      <td>0.499867</td>\n",
       "      <td>0.444621</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.882902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.495934</td>\n",
       "      <td>0.441416</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.883235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.495588</td>\n",
       "      <td>0.437631</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.883066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 08:27:36,059] Trial 7 finished with value: 0.495587557554245 and parameters: {'learning_rate': 5.009766817925159e-05, 'per_device_train_batch_size': 16, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 08:27:36,202 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152a31a015e84456bb49073f31d3706c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 25:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.493803</td>\n",
       "      <td>1.051355</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.707886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.413748</td>\n",
       "      <td>1.114586</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.715158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.516400</td>\n",
       "      <td>1.339164</td>\n",
       "      <td>1.137895</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.718933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.516400</td>\n",
       "      <td>1.258945</td>\n",
       "      <td>1.096515</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.728684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.516400</td>\n",
       "      <td>1.198144</td>\n",
       "      <td>1.079167</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.742267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.313800</td>\n",
       "      <td>1.144519</td>\n",
       "      <td>1.046024</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.750744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.313800</td>\n",
       "      <td>1.098721</td>\n",
       "      <td>0.997028</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.758393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.151000</td>\n",
       "      <td>1.062878</td>\n",
       "      <td>0.955772</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.764391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.151000</td>\n",
       "      <td>1.040964</td>\n",
       "      <td>0.931616</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.773694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.151000</td>\n",
       "      <td>1.033641</td>\n",
       "      <td>0.924323</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.778314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 08:53:09,997] Trial 8 finished with value: 1.0336408615112305 and parameters: {'learning_rate': 1.2275766321499958e-05, 'per_device_train_batch_size': 32, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05}. Best is trial 0 with value: 0.4852861762046814.\n",
      "2025-11-01 08:53:10,138 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning ibm-granite/granite-8b-code-instruct-4k...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccef839ca2c4e53a3eeadf20153fa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 22:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.449667</td>\n",
       "      <td>1.124329</td>\n",
       "      <td>408902.000000</td>\n",
       "      <td>0.708864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.531600</td>\n",
       "      <td>1.285001</td>\n",
       "      <td>1.122447</td>\n",
       "      <td>817804.000000</td>\n",
       "      <td>0.722069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.329200</td>\n",
       "      <td>1.126454</td>\n",
       "      <td>1.035548</td>\n",
       "      <td>1226706.000000</td>\n",
       "      <td>0.751335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.110700</td>\n",
       "      <td>0.948483</td>\n",
       "      <td>0.846474</td>\n",
       "      <td>1635608.000000</td>\n",
       "      <td>0.798463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.110700</td>\n",
       "      <td>0.813942</td>\n",
       "      <td>0.746100</td>\n",
       "      <td>2044510.000000</td>\n",
       "      <td>0.828347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.882300</td>\n",
       "      <td>0.740846</td>\n",
       "      <td>0.694734</td>\n",
       "      <td>2453412.000000</td>\n",
       "      <td>0.841983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.755300</td>\n",
       "      <td>0.703911</td>\n",
       "      <td>0.665239</td>\n",
       "      <td>2862314.000000</td>\n",
       "      <td>0.852093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.700100</td>\n",
       "      <td>0.682326</td>\n",
       "      <td>0.646669</td>\n",
       "      <td>3271216.000000</td>\n",
       "      <td>0.856262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.671284</td>\n",
       "      <td>0.636521</td>\n",
       "      <td>3680118.000000</td>\n",
       "      <td>0.857863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.668098</td>\n",
       "      <td>0.633040</td>\n",
       "      <td>4089020.000000</td>\n",
       "      <td>0.858500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 09:15:30,240] Trial 9 finished with value: 0.6680976152420044 and parameters: {'learning_rate': 1.0907720688848449e-05, 'per_device_train_batch_size': 16, 'r': 16, 'lora_alpha': 16, 'lora_dropout': 0.1}. Best is trial 0 with value: 0.4852861762046814.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error running PEFT pipeline: name 'model' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_18610/2702774244.py\", line 229, in peft_finetuning_pipeline\n",
      "    model=model,\n",
      "          ^^^^^\n",
      "NameError: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "lora_finetuning_pipeline(f\"{os.getenv('HF_USERNAME')}/jsp-code-to-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b366d3-90a2-4ff1-8501-dc963a96a2df",
   "metadata": {},
   "source": [
    "### Evaluate the candidate models\n",
    "Evaluate the candidate models using the following metrics / bechmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7fba3e-e2fe-40df-89bc-3c307ed54414",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Perform data\n",
    "###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641f134-8345-42c3-b7d7-173d35777470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
